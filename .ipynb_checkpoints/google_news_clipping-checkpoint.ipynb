{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 구글 뉴스 클리핑하기\n",
    "https://news.google.com/search?q=python&hl=ko&gl=KR&ceid=KR%3Ako\n",
    ">> search? 부분을 확인했다면, 로봇의 접근을 허용하고 있다는 뜻이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url=\"https://news.google.com\"\n",
    "search_url=base_url+\"/search?q=python&hl=ko&gl=KR&ceid=KR%3Ako\"\n",
    "resp=requests.get(search_url)\n",
    "html_src=resp.text\n",
    "soup=BeautifulSoup(html_src,'html.parser')\n",
    "\n",
    "#뉴스 아이템 블록 선택\n",
    "news_items=soup.select('div[class=\"xrnccd\"]')\n",
    "print(len(news_items))\n",
    "print(news_items[0])\n",
    "print(\"\\n\")\n",
    "\n",
    "#각 뉴스 아이템에서 링크,제목,내용,출처,등록일시 데이터를 파싱한다.\n",
    "for item in news_items[:3]: #앞에서부터 3개의 원소만을 대상으로 반복문을 적용한다.\n",
    "    link=item.find('a',attrs={'class':'VDXfz'}).get('href') #href 속성을 따로 추출하기 위해 get() 메서드 사용\n",
    "    news_link=base_url+link[1:]\n",
    "    #link는 ./articles/~로 시작하기 떄문에 .를 제거하기 위해 두 번째 문자부터 슬라이싱한다.\n",
    "    print(\"_____________________________\")\n",
    "    print(news_link)\n",
    "    \n",
    "    news_title=item.find('a',attrs={'class':'DY5T1d'}).getText()\n",
    "    print(\"_____________________________\")\n",
    "    print(news_title)\n",
    "    #a태그요소에 getText()메소드를 적용해서 텍스트부분을 추출한다.\n",
    "    \n",
    "    news_content=item.find('span',attrs={'class':'xBbh9'}).text \n",
    "    #span의 text 속성을 이용해서 텍스트부분을 추출한다.\n",
    "    print(\"_____________________________\")\n",
    "    print(news_content)\n",
    "    \n",
    "    news_agency=item.find('a',attrs={'class':'wEwyrc AVN2gc uQIVzc Sksgp'}).text\n",
    "    print(\"_____________________________\")\n",
    "    print(news_agency)\n",
    "    \n",
    "    news_reporting=item.find('time',attrs={'class':'WW6dff uQIVzc Sksgp'})\n",
    "    news_reporting_datetime=news_reporting.get('datetime').split('T')\n",
    "    #split으로 문자열의 날짜와 시간 부분을 나눈다.\n",
    "    news_reporting_date=news_reporting_datetime[0][:-1]\n",
    "    news_reporting_time=news_reporting_datetime[1][:-1]\n",
    "    print(\"_____________________________\")\n",
    "    print(news_reporting_date,news_reporting_time)\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    #앞의 코드를 이용해서 구글 뉴스 클리핑 함수 정의\n",
    "    def google_news_clipping(url,limit=5):\n",
    "        resp=requests.get(url)\n",
    "        html_src=resp.text\n",
    "        soup=BeautifulSoup(html_src,'html.parser')\n",
    "        \n",
    "        news_items=soup.select('div[class=\"xrnccd\"]')\n",
    "        \n",
    "        links=[]; titles=[]; contents=[]; agencies=[]; reporting_dates=[]; reporting_times=[];\n",
    "        \n",
    "        for item in news_items[:limit]:\n",
    "            link=item.find('a',attrs={'class':'VDXfz'}).get('href')\n",
    "            news_link=base_url+link[1:]\n",
    "            links.append(news_link)\n",
    "            \n",
    "            news_title=item.find('a',attrs={'class':'DY5T1d'}).getText()\n",
    "            titles.append(news_title)\n",
    "            \n",
    "            news_content=item.find('span',attrs={'class':'xBbh9'}).text\n",
    "            contents.append(news_content)\n",
    "            \n",
    "            news_agency=item.find('a',attrs={'class':'wEwyrc AVN2gc uQIVzc Sksgp'}).text\n",
    "            agencies.append(news_agency)\n",
    "            \n",
    "            news_reporting=item.find('time',attrs={'class':'WW6dff uQIVzc Sksgp'})\n",
    "            news_reporting_datetime=news_reporting.get('datetime').split('T')\n",
    "            #split으로 문자열의 날짜와 시간 부분을 나눈다.\n",
    "            news_reporting_date=news_reporting_datetime[0]\n",
    "            news_reporting_time=news_reporting_datetime[1][:-1]\n",
    "            reporting_dates=news_reporting_date\n",
    "            reporting_times=news_reporting_time\n",
    "            \n",
    "        result={'link':links,'title':titles,'content':contents,'agency':agencies,'date':reporting_dates,\\\n",
    "                'time':reporting_times}\n",
    "        return result\n",
    "    \n",
    "    #함수 실행하여 뉴스 목록 정리하기\n",
    "    news=google_news_clipping(search_url,2)\n",
    "    print(news)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
